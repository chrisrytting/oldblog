---
layout: post
title:  "Understanding Machines"
date:   2020-11-15 12:32:59 -0600
categories: jekyll update
---

Language models (LMs), which learn how to generate text by reading text and inferring its statistics, can seem to exhibit cognition when they produce novel and coherent English sentences. However, just what these models are doing is disputed. Are they copying, processing, thinking? A recent [paper](https://www.aclweb.org/anthology/2020.acl-main.463/) by Bender and Koller stipulates that LMs' mastery of linguistic form should not be conflated with these models' understanding just what is meant by that language. I refute a key claim of theirs, that "communicative intents" are extra-lingual, and argue that meaning as defined by Bender and Koller can be learned from form alone.

In the authors' definition, meaning is a relation between utterances and their "communicative intents", i.e. the intended consequence(s) of using language in the first place. The authors insist that these intents are external to language, and they might include the desires to "[convey] information to another person; or to ask them to do something; or simply to socialize". But while these examples can certainly take implicit form in a vague utterance, they certainly aren't extra-lingual, for they can be expressed in language even if they happen to not be. What intent cannot be expressed in language? The irony is not lost on me of expecting to have been provided linguistic representations of examples of extra-lingual entities. But of all the examples pointed to above, I can think of no utterance-intent pair where the former can't be clarified by an English specification of the latter. 

Intents themselves can be utterances. Natural language arises just for this purpose, as a matter of fact. The reason the English language has grown to include hundreds of thousands of words is that its speakers wanted articulated symbols--words--for experienced concepts that had none (i.e. were extra-lingual). So they came up with words, and every word is newer than what it stands for. Communicative intents are certainly a subset of these experiences for which we've constructed symbol-concept pairs. Thus, per the list above, if I want either to convey information to another person or to ask them to do something by articulating the utterance "I am hungry", it's simply not the case that I can't specify what I am saying such what I mean is communicated. Whether I want to convey information like "The reason I'm going to be grumpy and sluggish during this meeting is that my blood sugar is low" or make a request like "Will you agree to change our plans to go to dinner instead of the museum?" (which are certainly different intents for the same utterance), there is nothing stopping me from using my words.

Now, if the argument the authors wanted to make were actually that machines cannot know what is meant by "I am hungry" in the same way that humans do because machines don't chew, swallow, and digest food and thus cannot have experienced the sensation of hunger or fullness, the argument would be more sound. There is no evidence or reason to think that a computer could have experienced the human sensation of hunger, given its lack of hormones, blood sugar, enzymes, or other key links in the chain of human hunger. Missing such experience makes for a shallower understanding.

Similarly, if the argument were that mastering form is a necessary if not sufficient condition for understanding meaning, and machines cannot have achieved the latter because they often botch the former, then I would have to agree. No meaning-sensitive human would ever say, for example, that "a good way to light a fire is to use a dry cotton ball to cover the fuse!", as a widely acclaimed LM once was found to do (even though this sentence was apparently statistically likely). Missing form means missing meaning.

But both of these scenarios change the definition of meaning that Bender and Koller specify, into embodied experience or more robust concepts paired with symbols. Given their definition, the barrier to LMs understanding meaning is not that intents are extra-lingual and cannot be processed by a model limited to text. If linking a vague utterance to a more specific, clarifying intent is sufficient for understanding meaning, since meaning is a relation between utterances and intents, then understanding meaning _can_ be captured by learning form alone. If a successful LM learns to read the sentences "What should we do?" followed by "I am hungry" and expect something like "Okay, then, let's go to dinner!" instead of "I am sorry", then the lack of understanding is not due to a failure of the machine to produce human-like binding of utterances and communicative intents.

If the intents are intra-lingual, then LMs no longer are inhibited from learning meaning by reading form, since a set of training data can be as robust as necessary to represent the relation between utterances and their various explicit communicative intents.

Of course, in real life we will not have a dataset so robust, and probably no model could learn it if we did. I don't know what the solution is to learn symbol-concept sets robustly, since it's an unsolved problem, but the main roadblock is not that LMs cannot learn meaning from form alone, and this is the wrong critique of the hill we are climbing.
